## 데이터 기반 고객 세그먼테이션 및 맞춤형 마케팅 전략 수립: UCI Online Retail 분석

1. 머리말
* 1.1 프로젝트 목표
* 1.2 기획 배경
* 1.3 기대 효과

 
2. 고객 세그먼테이션
* 2.1 정의
* 2.2 유형
* 2.3 중요성


3. 분석 절차
* 3.1 데이터 수집
* 3.2 데이터 전처리
* 3.3 세그먼트 기준 설정
* 3.4 군집 분석 및 모델링
 

4. UCI, Online Retail Data Set을 이용한 고객 세그먼테이션 분석
* 4.1 데이터 수집 및 전처리
* 4.2 세그먼트 기준 설정
* 4.3 군집 분석 및 모델링
* 4.4 결과 해석
 

5. 마케팅 분석 및 전략
* 5.1 클러스터별 특성 분석
* 5.2 마케팅 전략 제안
  - Cluster 0 (VIP 고객)
  - Cluster 1 (이탈 가능 고객)
  - Cluster 2 (잠재 고객)
 

6. 결론 및 향후 계획
* 6.1 프로젝트 요약
* 6.2 한계점 및 어려웠던 부분

---

1. 머릿말
* 1.1  프로젝트 주제

 프로젝트는 고객 세그먼테이션의 정의를 명확히 하고, 이를 통해 데이터 기반의 고객 분류 및 세분화된 마케팅 전략 수립의 중요성을 전달하는 것에 첫 번째 목표를 두고 있다. 뿐만 아니라 앞서 설명한 것들을 기반으로 UCI Online Retail Data Set을 기반으로 고객 세그먼테이션을 통해 고객 그룹을 식별하고, 각 그룹에 맞춘 마케팅 전략을 제안하는 것을 두 번째 목표로 하고 있다. 따라서 이번 프로젝트의 주된 목적은 세그먼트를 나누는 여러 기준 중에서 RFM(Recency, Frequency, Monetary) 분석을 중심으로 군집화 알고리즘을 적용하여 고객을 효과적으로 분류하고, 각 세그먼트에 맞춘 마케팅 전략을 제안하는 것이다.


* 1.2 기획 배경

 오늘날 기업들은 더 이상 모든 고객에게 동일한 마케팅 전략을 적용하는 것에 만족하지 않는다. 고객 맞춤형 서비스에 대한 수요가 증가하면서, 고객 세그먼테이션을 통한 맞춤형 마케팅이 필수적인 시대가 되었기 때문이다. 특히, 전자상거래 산업에서는 고객 데이터를 분석해 충성 고객을 유지하고, 이탈 가능성 고객을 타겟팅해 재구매를 유도하는 것이 중요한 과제가 되었다. 필자는 UCI Online Retail Data Set은 이러한 분석을 수행하기 위한 적합한 데이터로, 고객의 구매 패턴을 바탕으로 세분화된 그룹을 식별하고, 각 그룹의 특성에 맞춘 효율적인 마케팅 전략을 수립할 수 있는 데 충분한 연습 데이터라고 보고 있어 이번 프로젝트를 기획하게 되었다.


* 1.3 기대 효과

 프로젝트를 통해 얻을 수 있는 기대 효과는 다음과 같다.
* 맞춤형 마케팅 전략: 고객 세그먼테이션을 통해 각 그룹의 특성에 맞춘 개인화된 마케팅 전략을 수립함으로써 고객 만족도를 향상시키고 마케팅 비용을 절감할 수 있다.
* 고객 유지율 증가: 이탈 가능성이 높은 고객을 조기에 식별하고, 그들을 대상으로 맞춤형 재구매 유도 전략을 펼침으로써 고객 유지율을 높일 수 있다.
* 충성 고객 관리: VIP 고객을 식별하고, 이들에게 차별화된 혜택과 로열티 프로그램을 제공함으로써 고객 충성도를 강화할 수 있다.
* 비즈니스 성장: 데이터를 기반으로 한 정확한 고객 세분화는 기업의 자원 배분을 최적화하고, 더 높은 투자 대비 수익률(ROI)을 달성할 수 있는 기반을 제공한다.

---

2. 고객 세그먼테이션
* 2.1 정의

 고객 세그먼테이션(Customer Segmentation)은 고객을 공통된 특성을 가진 다양한 그룹으로 나누는 분석 기법이다. 이처럼 고객을 공통된 특성을 가지는 다양한 그룹으로 나누는 이유는 맞춤형 마케팅을 위해서다. 즉, 비즈니스는 각 그룹의 특성이나 행동 패턴을 보다 잘 이해하고, 이를 바탕으로 맞춤형 마케팅 전략을 수립할 수 있기 때문이다. 좀 더 자세히 말하자면 다음과 같다. 

 고객 세그먼테이션의 주요 목표는 고객 데이터를 여러 그룹으로 세분화해 각 그룹의 특성에 맞는 마케팅 전략이나 비즈니스 전략을 수립하는 것이다. 세분화된 그룹들은 내부적으로는 유사한 특성을 가지지만, 다른 그룹과는 다른 행동 패턴 또는 속성을 보여준다. 따라서 고객 세그먼테이션을 잘 수행하면 기업은 각 고객군의 요구와 행동을 보다 잘 이해하고, 대량 마케팅, 세분화 마케팅, 틈새 마케팅 등 더 개인화된 경험을 제공하여 고객 만족도와 비즈니스 성과를 높일 수 있다. 예시로 A 쇼핑몰에서 아래와 같은 데이터가 있다고 가정을 하겠다. 성별, 나이, 연간 수익, 소비 점수가 주어졌을 때 이 데이터를 가지고 고객을 그룹화해야 한다.

![image](https://github.com/user-attachments/assets/59491c96-7d6e-439c-9778-25e007826240)

 간단하게 덴드로그램을 통해 데이터 포인트들 사이의 유사성 또는 거리를 바탕으로 군집의 수를 간편하게 결정할 수 있다. 단, 분석가의 판단에 의지하는 방법으로 주관적이다는 단점이 있다. 덴드로그램을 통해 군집 간 높이 차이가 큰 군집의 개수를 선택해 응집력이 높고 이질성이 큰 군집으로 구분할 수 있다. 아래 그림(1)을 보면 100~150 높이에 선을 그으면 5개의 군집으로 구분할 수 있다. 5개의 군집으로 구분한 후 그래프를 시각화하면 그림(2)와 같이 표현할 수 있다. cluster 1(이후 그룹 1, 2, 3, ... 으로 부르겠다.)은 연간 수입은 많지만 소비를 많이 하지 않는 그룹으로 볼 수 있다. 그룹 2는 소비와 연간 수입이 적절한 그룹으로 볼 수 있다. 그룹 3은 연간 수입도 많으며 소비도 많다. 그룹 4는 연간 수입이 적지만 소비가 많은 그룹이다. 그룹 5는 연간 수입, 소비 모두 적은 그룹이다. 마켓팅을 담당하는 사람에게 있어서 그룹3을 목표로 하는 것이 가장 좋은 전략일 수도 있다. 물론 그룹 4의 경우에도 판매 목적에서 좋은 대상일 수 있지만 윤리적으로 제외할 수 있는 그룹으로 분류할 수 있다. 예시를 통해 간단하게 알아 본 것처럼 고객 세그먼테이션은 맞춤형 마케팅 전략에 있어서 중요한 정보를 제공할 수 있다.

 ![image](https://github.com/user-attachments/assets/c9c6d474-fda4-41b5-b863-cbebce89d9ac)

* 2.2 유형

 고객은 점점 차별화되고 개인화된 나만의 상품과 나만을 위한 서비스 즉, 개인화된 서비스를 원한다. 이에 맞춰 기업은 고객을 더 세분화하여 이전보다 더 차별화된 전략을 펼치려고 하고 있다. 고객을 세분화하는 기준은 매우 다양하며, 주로 인구통계적 특성, 행동적 특성또는 RFM 세그먼테이션에 따라 구분할 수 있다.

 * 1. 인구 통계적 특성(연령, 성별 등)

 개인화에 있어서 인구 통계적 특성은 유용한 자료이다. 인구 통계적 세그먼테이션은 고객의 인구학적 정보를 기준으로 고객을 분류한다. 가장 보편적인 방법이며 이해하기 쉽다. 특히, 데이터를 쉽게 얻을 수 있기 때문에 가장 많이 사용되는 방법이다. 주요 기준으로는 다음과 같다.
  * 연령
  * 성별
  * 소득 수준
  * 교육 수준
  * 직업
  * 가족 구성

 연령에 따라 선호하는 제품이 다르며, 가격 또한 차이가 날 수 있다. 예를 들어 10대, 20대의 경우 30대, 40대에 비해 화려한 것을 또는 가성비 있는 제품을 선호할 수 있다. 성별 역시 남성과 여성에 따라 필요로 하는 또는 선호하는 제품이 다르다. 마찬가질 소득 수준, 교육 수준, 직업, 가족 구성에 따라 선호하는 또는 필요로 하는 제품이 다르기 때문에 주요 기준으로 볼 수 있다.

 * 2. 행동적 특성(구매 패턴, 웹사이트 방문 빈도 등)

 개인화에 있어서 행동적 특성 역시 유용한 자료이다. 행동적 세그먼테이션은 고객의 구매 패턴, 웹사이트 사용 패턴 등에 따라 고객을 분류한다. 주요 기준으로는 다음과 같다.
  * 구매 빈도
  * 구매 시점
  * 브랜드 충성도
  * 프로모션 민감도

 각각 구매 빈도에 따라 특정 제품을 자주 구매한다면 비슷한 상품을 추천할 수 있다. 구매 시점에 따라 특정 제품과 연관된 상품을 추천할 수도 있다. 특정 브랜드만을 구입하는 고객에게는 특정 브랜드만을 추천하거나 반대의 경우엔 여러 브랜드를 추천할 수 있다. 또는 할인이나 프로모션에 민감할 경우 할인하는 제품을 추천하는 등 개인화를 할 수 있다.

 * 3. RFM 세그먼테이션

 RFM은 Recency(최근 구매일), Frequency(구매 빈도), Monetary Value(구매 금액)를 기준으로 고객을 세분화하는 방법이다. 고객의 가치를 측정하고 충성도 높은 고객을 식별하는 데 유용하다. RFM의 구성 요소는 다음과 같다.

   * Recency(최근 구매일)
 
     고객이 가장 최근에 구매한 날짜를 의미하며, 최근 구매일이 가까울 수록 높은 점수를 가진다.
 
   * Frequency(구매 빈도)
 
     고객이 일정 기간 동안 얼마나 자주 구매했는가를 의미하며, 구매 횟수가 많을 수록 높은 점수를 가진다.
 
   * Monetary Value(구매 금액)
 
     고객이 일정 기간 동안 얼마를 지출했는가를 의미하며, 구매 금액이 클 수록 높은 점수를 가진다.

 이 외에도 심리적 세그먼테이션, 지리적 세그먼테이션 등이 있다. 필자는 UCI, Online Retail Data Set을 기반으로 한 고객 세그먼테이션 분석을 진행할 것이며 주요 방법으로 RFM 세그먼테이션을 이용할 것이다.

* 2.3 중요성

 앞서 말했듯 최근 많은 고객들은 개인화된 서비스를 원하고 있다. 이에 기업 역시 발맞추기 위해 고객을 더 세분화하려고 노력하며 더 차별화된 전략을 펼치려 하고 있다. 이처럼 차별화된 전략을 펼치기 위해서는 개인화된 마케팅 전략을 수립할 수 있는 데이터가 필요하며, 고객 세그먼테이션은 개인화된 마케팅 전략을 수립할 수 있는 기초 데이터를 제공하는 주요 방법이다. 고객의 요구가 다를 때 동일한 전략을 적용하는 것은 비효율적이며, 각각의 세그먼트에 맞춘 전략을 수립함으로써 얻을 수 있는 이점은 다음과 같다.
 
 * 1. 맞춤형 마케팅 전략

 기업은 각 고객 세그먼트에 대해 개인화된 경험을 제공할 수 있다. 예를 들어, 자주 구매하는 고객에게는 VIP 혜택을 제공하고, 할인에 민감한 고객에게는 프로모션을 제안하는 등 맞춤 전략이 가능하다.
 
 * 2. 고객 유지율 증가

 고객이 왜 이탈하는지에 대한 인사이트를 제공할 수 있다. 이탈하기 쉬운 고객 그룹을 식별한 후, 이탈하기 전에 이벤트를 통해 고객을 유지할 수 있다. 예를 들어, 오랫동안 구매하지 않은 고객에게 할인 쿠폰을 제공하거나, 고객 만족도를 높이기 위한 추가적인 서비스를 제공할 수 있다.

 * 3. 자원 최적화 및 비용 절감

 모든 고객에게 동일한 마케팅 전략을 사용하는 대신, 가장 수익성이 높은 그룹에 집중하여 자원을 효율적으로 사용할 수 있다. 이러한 방법은 마케팅 비용을 줄이고, 동시에 더 높은 ROI(투자 대비 수익률)를 달성할 수 있다.
 
 * 4. 제품 개발 및 혁신

 각 세그먼트의 특성을 분석하여 신제품 개발에 필요한 아이디어를 얻을 수 있다. 특정 세그먼트에서 인기가 있는 제품을 분석해 새로운 기능이나 서비스 개선을 제안할 수 있다.

---

3. 분석 절차

 아래와 같은 방법을 통해 UCI에서 제공하는 Online Retail Data Set 분석을 진행할 예정이다. 데이터 수집의 경우 데이터를 직접 수집할 필요 없이 UCI(https://archive.ics.uci.edu/dataset/502/online+retail+ii)에서 다운받을 수 있다. 이후 데이터 전처리, 세그먼트기준 설정, 군집 분석 및 모델링을 진행할 것이다.

* 3.1 데이터 수집

 고객 데이터를 수집하는 것이 고객 세그먼테이션을 진행하기 위한 첫 번째 단계이다. 데이터는 고객의 구매 기록, 웹사이트 활동 로그, CRM(Customer Relationship Management) 데이터 등 다양한 출처에서 다양한 정보를 수집할 수 있다. 데이터는 가능한 한 풍부하고 정확해야 세그먼테이션의 결과가 더 신뢰성을 가질 수 있다.

* 3.2 데이터 전처리

 수집된 데이터를 분석 가능한 형식으로 만들기 위해 전처리 과정이 필요하다. 여기에는 결측값 처리, 이상치 제거, 데이터 정규화 등이 포함된다. 이 단계에서 데이터를 정리하고, 세그먼테이션을 위한 변수를 선택한다.

* 3.3 세그먼트 기준 설정

 세그먼트를 나눌 기준을 설정하는 것은 중요한 단계이다. 예를 들어, 연령, 소득, 구매 빈도, 라이프스타일 등의 기준을 선택할 수 있다. 이때, 비즈니스 목표와 고객 니즈에 맞는 기준을 선택하는 것이 중요다.

* 3.4 군집 분석 및 모델링

 고객 데이터를 기반으로 세그먼트를 나누기 위해서는 군집 분석을 수행해야 한다. 군집 분석은 고객 간의 유사성을 분석해 여러 그룹으로 나누는 비지도 학습 방식이다. 주요 군집 분석 기법은 다음과 같다.

  * K-Means

  쉽고 직관적인 알고리즘으로 가장 일반적으로 사용되는 군집 분석 방법으로 고객을 미리 설정한 개수의 그룹으로 나누는 방식이다. 하지만 거리 기반으로 군집 중심점을 이동시키면서 군집화를 수행하기 때문에 복잡한 구조를 가지는 데이터 세트에 적용하는 데에 한계가 있을 수 있다.

  * Mean Shift
  
  K-Means와 유사하지만 거리 중심이 아닌 밀도가 가장 높은 쪽으로 군집 중심점을 이동하면서 군집화를 수행한다. 특정 개체를 구분하거나 움직임을 추적하는 데 뛰어난 역할을 수행하는 알고리즘으로 컴퓨터 비전 영역에서 자주 사용된다.
  
  * GMM
  
  데이터가 여러 개의 가우시안 분포를 가진 데이터 집합들이 섞여서 생성된 것이라는 가정하에 군집화를 수행하는 알고리즘이다. 전체 데이터 세트에서 서로 다른 정규 분포 형태를 추출해 다른 정규 분포를 가진 데이터 세트를 각각 군집화한다. 다양한 데이터 세트에 잘 적용될 수 있지만 시간이 오래걸린다는 단점이 있다.
  
  * DBSCAN
  
  밀도 기반 군집화의 대표적인 알고리즘으로 이상치가 많거나 군집의 크기가 일정하지 않은 데이터를 다룰 때 유용합니다. 데이터의 분포가 기하학적으로 복잡한 데이터 세트에도 효과적인 군집화가 가능하다.

---

4. UCI, Online Retail Data Set을 기반으로 한 고객 세그먼테이션 분석

* 4.1 데이터 수집 및 전처리

Online Retail Data Set은 UCI에서 제공하는 데이터이다.

online_retail_II.xlsx = [Daqing Chen, Online Retail data II, UCI, 2019년 수정, 2024년 9월 20일 접속, https://archive.ics.uci.edu/dataset/502/online+retail+ii%EF%BB%BF]

다운 받은 파일은 xlsx 파일로 판다스의 read_excel 함수를 이용해 DataFrame 형태로 로드할 수 있다. 로드한 파일은 그림(3)과 같다. 총 8개의 column을 가지고 있으며 총 데이터는 525,461개의 데이터이다.

![image](https://github.com/user-attachments/assets/e540b367-bd9b-402c-893d-46c1155c8ae9)

그림(3)에 나타난 column이 의미하는 것은 다음과 같다.

  * Invoice: 주문 번호, C로 시작하면 주문 취소
  * StockCode: 제품 코드
  * Description: 제품 설명
  * Quantity: 주문 제품 건수
  * invoiceDate: 주문 일자
  * Price: 제품 단가
  * CustomerID: 고객 번호
  * Country: 국가명(주문 고객의 국적)

 행과 열의 크기, 컬럼명, 컬럼을 구성하는 값의 자료형 등을 확인하기 위해 .info() 함수를 통해 데이터에 대한 전반적인 정보를 확인하면 그림(4)와 같다. Invoice, StockCode, Description, Country는 앞서 column을 설명한 데로 object 타입이다. Quantity, price, Customer ID 역시 각각 int, float 타입이다. InvoiceDate는 날짜를 나타는 column으로 datetime 타입임을 확인할 수 있다. 특히, Customer ID의 경우 Null 값이 많이 있다. 총 525.461개의 데이터 중에서 10만 7천 여건의 데이터가 Null이다. 하지만 고객 세그먼테이션을 진행하기 때문에 식별될 수 없는 고객은 필요가 없는 데이터이므로 삭제하겠다. Customer ID 외에도 다른 column에서도 오류 데이터가 존재할 수 있기 때문에 확인이 필요하다.

![image](https://github.com/user-attachments/assets/c4fa3690-86c5-420d-9e06-1347ece5665b)

 다음으로 .describe() 하수를 통해 데이터의 컬럼별 요약 통계량을 확인하겠다. 그림(5)와 같이 object column을 제외한 column들에 대한 통계를 확인할 수 있다. Quantity의 경우 - 값이 존재하는 것을 확인할 수 있다. 또한, Price의 경우 역시 - 값이 존재하는 것을 확인할 수 있다.

 ![image](https://github.com/user-attachments/assets/5003adfa-7a83-49e7-bf5a-e2004ed627dd)

  그림(5)에서 확인했던 Quantity, Price에 대해서 확인을 해보겠다. Quantity의 경우 0보다 작은 데이터는 오류 데이터가 아닌 반환 즉, 주문 취소를 의미하는 것이다. 하지만 고객 세그먼테이션에서의 혼란, 정확한 매출 분석을 위해 제거하는 것이 맞다고 판단했다. 이유는 다음과 같다.

  ![image](https://github.com/user-attachments/assets/398d3e00-6cc7-4b2b-9370-b26d93972a9e)

   그림(6)을 보면 고객이 물건을 구매한 기록이 있고 마지막 부분에서 환불 또는 취소한 것을 확인할 수 있다. 보통 취소를 하면 해당 금액 만큼 결제를 하고 그 금액을 취소하지만 기록에는 결제한 금액은 없고 환불 또는 취소한 기록만 있다. 그림(6)에 등장하는 고객은 주문 건수가 상당히 많지만 환불 또는 취소한 값이 너무 커서 Customer ID로 그룹화해서 분류했을 때 총 결제 금액이 -인 것으로 분류된다. 따라서 제거하기로 했다. 추가적으로 그림(7)을 보면 TEST, ADJUST가 있다. ADJUST로 -를 한다해도 테스트 결제가 더 많으며, 분석에 있어서 필요가 없는 데이터이기 때문에 StockCode에 TEST, ADJUST가 포함된 행은 삭제하기로 했다.

![image](https://github.com/user-attachments/assets/b272e27b-ef46-46a3-8ad3-8da1c5dd0395)

 다음으로 Price가 - 값인 데이터는 그림(8)을 통해 확인해보면 총 3개가 있다. 하지만 Customer ID가 없는 데이터라 Cutomer ID가 Null 값인 데이터를 삭제하면 자동으로 삭제될 행이기에 따로 처리는 하지 않으려고 한다.

 ![image](https://github.com/user-attachments/assets/c146c1b7-fd9b-4737-9b76-9bad76e04dee)

  Country의 경우 총 40개의 국가가 있다. 지리적 세그먼테이션을 진행할 수 있지만 현재 가진 데이터에서 지리적 세그먼 테이션을 진행할 수 있는 부분은 Country 밖에 없다. 따라서 40개 국가 전체를 대상으로 세그먼테이션을 진행하는 것 보다 데이터의 대부분을 차지하는 상위 5개의 국가에 대해서 진행하려고 한다.
```
United Kingdom          485852
EIRE                      9670
Germany                   8129
France                    5772
Netherlands               2769
```
 Customer ID가 Null인 데이터는 삭제하고 상위 5개의 Country만 추출을 진행하고 .info() 함수를 확인하면 그림(9)과 같다.

![image](https://github.com/user-attachments/assets/43e602ac-cede-4354-9048-4cbc0c684671)

 총 395,279개의 데이터를 가지고 있고 .isna().sum() 코드를 통해 확인했을 때 Null 값이 없는 것을 확인할 수 있다. 다음으로 Invoice와 StockCode를 확인해보겠다. 온라인 쇼핑몰로 물건을 구매했을 때 한 나의 주문 번호에 하나의 상품이 매칭되어 있거나 여러 물건을 샀을 때 하나의 주문 번호에 여러 물건이 매칭되어 있다. 따라서 평균적으로 한 주문에서 특정 상품이 몇 번씩 구매되었는지 확인해보려고 한다.

![image](https://github.com/user-attachments/assets/6ac294d5-c10c-4b51-80a1-f44667ea71e8)

그림(10)와 같이 확인할 수 있다. 즉, 대부분 한 번 구매할 때 특정 상품을 하나 씩  여러 상품을 같이 구매하는 것처럼 보인다. 그림(11)을 보면 대부분이 특정 상품을 한 나씩 구매하지만 극히 일부에서 동일한 상품을 여러 개를 구매한다는 것을 알 수 있다.

![image](https://github.com/user-attachments/assets/227d38ee-9868-4976-9135-6aa0338def15)


* 4.2 세그먼트 기준 설정

세그먼트는 기본적으로 RFM 기반으로 진행할 것이다. 특히, 이번 분석에서는 상위 5개 Country 전체를 대상으로 고객 세그먼테이션을 진행할 것이다.

고객 세그먼테이션 군집화를 RFM 기반으로 수행하기 위해서는 데이터 가공이 필요하다. RFM은 Recency(최근 구매일), Frequency(구매 빈도), Monetary Value(구매 금액)를 기준으로 고객을 세분화하는 방법으로 먼저 주문 금액 데이터 만들기 위해 Price와 Quantity를 이용하겠다. Price와 Quantity를 곱해서 price_amount column을 만들었다. 결과는 그림(12)과 같다. 

![image](https://github.com/user-attachments/assets/d985d9af-8b0c-4031-87c1-18bdd72060c3)

새롭게 만든 price_amount를 .describe() 함수를 통해 확인해보면 아래와 같이 확인할 수 있다. 특정 제품에 대해 많은 수량을 구매한 것을 확인할 수 있다. 이를 통해 해당 데이터가 개인 고객의 주문뿐만 아니라 소매점의 주문이 함께 포함된 데이터라는 것을 확인할 수 있다. 즉, 주문을 많이한 고객이 총 사용 금액이 많을 수는 있지만 주문이 가장 많은 고객이 사용 금액이 가장 많은 것은 아니라는 것을 확인할 수 있다.
```
count    395279.000000
mean         21.210888
std          75.075768
min           0.000000
25%           4.950000
50%          11.700000
75%          19.500000
max       15818.400000
Name: price_amount, dtype: float64
```
 다음으로 RFM에서 Recency(최근 구매일)를 구해보겠다. 하지만 현재 날짜와 데이터에서의 날짜는 차이가 많이 나기 때문에 13년 9개월을 더해서 최근 날짜로 만들고자 한다. 날짜를 조정하면 그림(13)와 같이 2023-09-01부터 2024-09-09날짜로 조정된다.

 ![image](https://github.com/user-attachments/assets/bea942dc-ef87-465c-be06-261e242de43a)

 Recency(최근 구매일)는 개별 고객 당 가장 최근의 주문이다. 그래서 그림(13)에서 보여지는 것 처럼 데이터 값의 특성으로 가공이 필요하다. Recency(최근 구매일)는 고객이 가장 최근에 주문한 날짜를 기반으로 한다. 따라서 오늘 날짜를 기준으로 가장 최근 주문 일자를 뺀 날짜이다. 그래서 날짜를 13년 9개월을 더해 조정을 한 것이다. 결과적으로 InvoiceDate에서 현재 날짜를 빼면 구매일로부터 얼마나의 시간이 지났는지 확인할 수 있다.

 다음으로 Frequency(구매 빈도)를 구해보겠다. 고객별 주문 건수로 Customer ID로 goupby()를 해서 Invoice의 count()를 통해 구할 수 있다. 즉, Customer Id 별로 얼마나 많이 구매했는 가를 구하는 것이다. 그림(14)과 같이 확인할 수 있다.

 ![image](https://github.com/user-attachments/assets/4642e86c-939a-49d2-9a0b-c4946ad7fc2c)

 마지막으로 Monetary Value(구매 금액)를 구해보겠다. Monetary Value(구매 금액)는 고객별 주문 금액으로 Customer ID로 goupby()를 해서 price_amount의 sum()을 통해 구할 수 있다. 그림(15)와 같이 확인할 수 있다.

 ![image](https://github.com/user-attachments/assets/47a33a23-9aeb-46b3-9b7f-876adaec05d3)

 최종적으로 Quantity, Price, Description, StockCode, InvoiceDate, Invoice, price_amount, Country column을 삭제하면 그림(16)와 같이 4개의 column만 남는다. 또한, 중복된 행이 사라져서 395,279개의 행이 4104개의 행으로 줄어든 것을 확인할 수 있다. 이렇게 Recency(최근 구매일), Frequency(구매 빈도), Monetary Value(구매 금액)을 모두 생성했다.

 ![image](https://github.com/user-attachments/assets/0825b546-869e-479c-96ba-cdf26c20502d)

군집 분석 및 모델링 단계로 넘어가기 전에 확인할 것이 있다. 아래 결과와 같이 price_amount를 .describe() 함수를 통해 확인한 것을 통해 특정 제품에 대해 많은 수량을 구매한 것을 확인할 수 있었다. 이러한 소매점의 주문이 함께 포함된 데이터는 주문 횟수와 주문 금액에서 개인 고객 주문과 큰 차이를 보여준다. 이러한 왜곡된 데이터 분포도를 가진 데이터는 군집화가 한 쪽으로 집중되는 현상이 발생하게 된다. 이를 시각화를 통해 확인해 보겠다.
```
count    395279.000000
mean         21.210888
std          75.075768
min           0.000000
25%           4.950000
50%          11.700000
75%          19.500000
max       15818.400000
Name: price_amount, dtype: float64
```
![image](https://github.com/user-attachments/assets/0d3e28a8-4bb5-4c68-82e9-c4d20e35ae60)

그림(17)와 같이 왜곡된 데이터 값 분포도를 보여준다. 특히, 구매 빈도, 구매 금액이 특정 범위에 몰려있다. 수치를 통해 확인을 해보겠다.

![image](https://github.com/user-attachments/assets/33e54639-356f-44e1-a3ce-c2aaff0ad76b)

그림(18)에서 Recencu(최근 구매일)을 보면 평균이 101.6이지만 50%인 63보다 매우 크게 차이가 난다. 또한, max가 385이지만 75%가 147로 매우 크게 차이가 난다. Frequency(구매 빈도), Monetary(구매 금액) 역시 차이가 크게 난다. 이러한 데이터 세트는 왜곡 정도가 심하기 때문에 앞에서 설명했던 K-Means 군집을 적용했을 때 변별력이 떨어질 수 있다.

* 4.3 군집 분석 및 모델링

 군집화를 진행하기 전에 데이터를 StandardScaler를 이용해 평균과 표준편차를 재조정한 후 K-평균을 수행해 보겠다.
```
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples

scaling_feature = ['Recency','Frequency','Monetary']

scaler = StandardScaler()
X_features_scaled = scaler.fit_transform(cust_df[scaling_feature])

kmeans = KMeans(n_clusters=3, random_state=0)
labels = kmeans.fit_predict(X_features_scaled)
cust_df['cluster_label'] = labels

print('실루엣 스코어는 : {0:.3f}'.format(silhouette_score(X_features_scaled,labels)))
```
위와 같이 진행했을 때 실루엣 스코어는 0.581이 나왔다. 실루엣 스코어는 클러스터링 성능을 평가하는 지표로, 데이터 포인트가 잘 클러스터링되었는지를 측정하는 방법으로 클러스터 간의 분리가 잘 되었는지, 같은 클러스터 내 데이터 포인트들이 얼마나 가깝게 모여 있는지를 나타낸다.

 1에 가까울수록 클러스터 간의 분리가 명확하고, 같은 클러스터 내 데이터들이 서로 가깝게 잘 모여 있음을 의미한다. 0에 가까울수록 클러스터 간의 경계가 명확하지 않으며, 클러스터링이 모호함을 나타낸다. 음수(-1에 가까울수록) 데이터가 잘못된 클러스터에 속해 있음을 의미하며, 클러스터링의 품질이 좋지 않음을 나타낸다.

따라서 결과가 0.581이라는 것은 클러스터링이 비교적 잘 되었음을 보여준다. 이는 클러스터 간의 분리와 클러스터 내의 응집도가 적절하다는 의미이다. 결과를 시각화해서 보겠다.

![image](https://github.com/user-attachments/assets/e21cd317-ebc4-48c5-91d4-34e086de700a)

그림(19)를 보면 군집을 2개로 나눴을 때 0과 1의 분류가 제대로 이루어지지 않은 것을 확인할 수 있다. 뿐만 아니라 군집이 3개 이상일 때부터 군집 2개보다는 잘 나눠지지만 너무 작은 군집이 만들어지고 있다. 위의 실루엣 계수 역시 매우 작은 것을 확인할 수 있다. 이렇게 결과가 나타나는 이유는 앞에서 설명했듯 소수의 데이터가 군집화가 한 쪽으로 집중되게 만들기 때문이다. 이렇게 확인해 본 결과 실루엣 스코어의 절대치가 중요한 것은 아니라는 것을 확인할 수 있다.

 이러한 데이터 세트를 분리하고 숨어 있는 새로운 집단을 발견하는 것이 군집화의 목표이다. 즉, 군집 내의 데이터 값을 분석하고 이해함으로써 집단에 새로운 의미를 부여하는 것이 군집화의 목표이다. 따라서 데이터의 왜곡정도를 낮추고 다시 분석을 할 예정이다.

 왜곡 정도를 낮추기 위해 자주 사용되는 방법은 데이터 값에 log(로그)를 적용하는 로그 변환이다. 현재 데이터 세트는 음수를 값을 모두 제거했다. 따라서 로그 변환을 적용해도 괜찮으며 박스 콕스(Box-Cox) 변환 등을 사용하는 것도 좋은 방법이다. 필자는 위에서 했던 방법을 그대로 로그 변환을 사용했다. 
```
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples

cust_df['Recency_log'] = np.log1p(cust_df['Recency'])
cust_df['Frequency_log'] = np.log1p(cust_df['Frequency'])
cust_df['Monetary_log'] = np.log1p(cust_df['Monetary'])

# Log Transformation 데이터에 StandardScaler 적용
X_features = cust_df[['Recency_log','Frequency_log','Monetary_log']].values
X_features_scaled = StandardScaler().fit_transform(X_features)

kmeans = KMeans(n_clusters=3, random_state=0)
labels = kmeans.fit_predict(X_features_scaled)
cust_df['cluster_label'] = labels

print('실루엣 스코어는 : {0:.3f}'.format(silhouette_score(X_features_scaled,labels)))
```
그림(19)에 비해 균일하게 군집화가 구성된 것을 확인할 수 있다. 다음으로 엘보우 방법을 통해 최적의 클러스터 개수를 찾아 보겠다. 각 클러스터 수에서의 Sum of Squared Distances(SSD)를 계산하고 그 변화를 확인하여 몇 개의 클러스터가 적절한지 판단하는 방법으로 그림(21)처럼 확인할 수 있다. 그래프에서 SSD가 급격히 감소하다가 더 이상 큰 차이가 나지 않고 완만해지는 지점이 적절한 클러스터 수를 나타낸다. 따라서 그림(21)을 통해 본다면 3이 가장 적절한 클러스터 개수로 볼 수 있다.

![image](https://github.com/user-attachments/assets/73dc88ce-33cd-436d-b0b8-bff64fa6b934)

* 4.4 결과 해석

 앞서 군집화에 대한 결과를 해석하겠다. 군집화를 진행했을 때 가장 적절한 군집화의 개수는 3개이다. 따라서 Cluster 0, Cluster 1, Cluster 2로 나눌 수 있다. 하지만 단순히 군집화를 통해서는 Cluster 0, Cluster 1, Cluster 2가 무엇을 의미하는지는 알 수 없다. 따라서 Cluster 0, Cluster 1, Cluster 2가 무엇을 의미하는지 이해하기 위해서 클러스터별 특성을 분석하겠다.

![image](https://github.com/user-attachments/assets/411c987b-70e5-4a57-b24e-5cabf11964e3)

그림(22)와 같이 로그 변환한 값과 이전의 값 그리고 클러스터 레이블이 데이터프레임에 담겨있는 것을 알 수 있다. 이를 기반으로 특성을 분석하면 그림(23)과 같이 확인할 수 있다. 각각 Cluster에 대해 평균, 최소, 최대를 통해 각각 어떤 군집인지 이해할 수 있다.

![image](https://github.com/user-attachments/assets/fa6522bd-10a2-4e16-bb59-21f15b4f6634)

cluster 0, Recency는 고객들은 평균적으로 34일 전에 마지막으로 구매를 했다. 최소값은 11일, 최대값은 256일로, 구매한 지 오래되지 않은 고객들이 다수 포함되어 있다. Frequency는 고객들은 평균적으로 251번 구매한 경험이 있다. 최소 16번, 최대 5567번 구매한 기록이 있으며, 매우 자주 구매하는 고객들이다. 그리고 Monetary는 고객들은 평균적으로 5718.99 달러를 소비했다. 최소 507.56 달러, 최대 349,164.35 달러로, 구매 금액이 매우 높은 고객들이다. cluster 1, Recency는 고객들은 평균적으로 203일 전에 마지막으로 구매했다. 최소 14일, 최대 385일로, 오랜 시간 동안 구매하지 않은 고객이 많다. Frequency는 고객들은 평균적으로 16.93번 구매했으며, 최소 1번, 최대 94번 구매 기록이 있다. 구매 빈도가 낮다. 그리고 Monetary는 고객들은 평균적으로 295.40 달러를 소비했으며, 최소 1.55 달러, 최대 10,953.50 달러로, 구매 금액이 적은 편이다. cluster 2, Recency는 고객들은 평균적으로 72일 전에 마지막으로 구매했다. 최소 11일, 최대 339일로, 최근에 구매한 고객도 있고, 어느 정도 시간이 지난 고객도 포함되어 있다. Frequency는 고객들은 평균적으로 51.71번 구매했으며, 최소 1번, 최대 259번 구매 기록이 있다.그리고 Monetary는 클러스터의 고객들은 평균적으로 884.93 달러를 소비했으며, 최소 64.49 달러, 최대 10,877.18 달러로, 구매 금액은 중간 정도이다.

---

5. 마케팅 분석 및 전략
* 5.1 클러스터별 특성 분석

* cluster 0

  Recency (평균: 34.47): 이 클러스터의 고객들은 평균적으로 34일 전에 마지막으로 구매를 했다. 최소값은 11일, 최대값은 256일로, 구매한 지 오래되지 않은 고객들이 다수 포함되어 있다.
  Frequency (평균: 251): 이 고객들은 평균적으로 251번 구매한 경험이 있다. 최소 16번, 최대 5567번 구매한 기록이 있으며, 매우 자주 구매하는 고객들이다.
  Monetary (평균: 5718.99): 이 클러스터의 고객들은 평균적으로 5718.99 달러를 소비했다. 최소 507.56 달러, 최대 349,164.35 달러로, 구매 금액이 매우 높은 고객들이다.
  종합: VIP 고객. 구매 빈도가 매우 높고, 지출 금액도 상당히 크다. 충성 고객으로 분류할 수 있으며, 로열티 프로그램이나 특별한 혜택을 제공하는 마케팅 전략 필요.

* cluster 1

  Recency (평균: 203.91): 이 클러스터의 고객들은 평균적으로 203일 전에 마지막으로 구매했다. 최소 14일, 최대 385일로, 오랜 시간 동안 구매하지 않은 고객이 많다.
  Frequency (평균: 16.93): 이 고객들은 평균적으로 16.93번 구매했으며, 최소 1번, 최대 94번 구매 기록이 있다. 구매 빈도가 낮다.
  Monetary (평균: 295.40): 이 클러스터의 고객들은 평균적으로 295.40 달러를 소비했으며, 최소 1.55 달러, 최대 10,953.50 달러로, 구매 금액이 적은 편이다.
  종합: 이탈 가능성이 높은 고객으로 볼 수 있다. 구매한 지 오래되었고, 구매 빈도와 금액도 낮다. 이탈 방지를 위해 특별 할인, 재구매 유도 캠페인 등의 마케팅 전략이 필요.

* cluster 2

  Recency (평균: 71.99): 이 클러스터의 고객들은 평균적으로 72일 전에 마지막으로 구매했다. 최소 11일, 최대 339일로, 최근에 구매한 고객도 있고, 어느 정도 시간이 지난 고객도 포함되어 있다.
  Frequency (평균: 51.71): 이 고객들은 평균적으로 51.71번 구매했으며, 최소 1번, 최대 259번 구매 기록이 있다.
  Monetary (평균: 884.93): 이 클러스터의 고객들은 평균적으로 884.93 달러를 소비했으며, 최소 64.49 달러, 최대 10,877.18 달러로, 구매 금액은 중간 정도이다.
  종합: 잠재 고객으로 볼 수 있다. 구매 빈도가 중간 정도이며, 구매 금액도 낮지 않다. 재구매를 유도하여 VIP 고객으로 전환할 가능성이 있는 그룹이다. 할인 혜택, 맞춤형 추천 등을 제공하여 재구매를 촉진할 수 있는 마케팅 전략 필요.

* 요약

 Cluster 0은 VIP 고객으로 분류할 수 있다. 높은 구매 빈도와 큰 금액을 소비한 고객들임을 확인할 수 있었다.  따라서 이 그룹에 속하는 고객들에게는 특별 혜택과 충성 고객 관리가 필요하다. Cluster 1은 이탈 가능 고객으로 분류할 수 있다. 오랜 기간 동안 구매하지 않았고, 구매 빈도와 금액도 낮은 고객들임을 확인할 수 있었다. 따라서 이 그룹에 속하는 고객들에게는 이탈 방지 캠페인을 통한 재유입이 필요하다. Cluster 2는 잠재 고객으로 분류할 수 있다. 중간 정도의 구매 빈도와 금액을 가지고 있어 재구매를 촉진할 수 있는 고객들임을 확인할 수 있었다. 따라서 적극적인 마케팅으로 더 큰 가치를 창출할 수 있다.

* 5.2 마케팅 전략 제안

* Cluster 0 (VIP 고객):

  로열티 프로그램 도입
  프리미엄 서비스 제공 (VIP 전용 서비스, 추가 혜택)
  개인화된 상품 추천 및 고객 만족도 조사

* Cluster 1 (이탈 가능 고객):

  이탈 방지 마케팅: 이메일 캠페인, 타겟 맞춤형 할인 제공
  장기 미사용 고객을 위한 프로모션 (특별 할인 또는 쿠폰)
  재참여 유도 이벤트 (한정 기간 특별 혜택)

* Cluster 2 (잠재 고객):

  재구매 유도: 구매 빈도와 금액을 늘릴 수 있는 프로모션 제공
  제품 추천: 관심 상품을 바탕으로 개인화된 추천 제공
  적립금, 할인 쿠폰 제공

---

6. 결론
* 6.1 프로젝트 요약

 본 프로젝트는 UCI Online Retail Data Set을 활용하여 고객 세그먼테이션을 수행하고, 이를 기반으로 고객을 그룹화한 후 세분화된 마케팅 전략을 제안하는 과정으로 이루어졌다. RFM 분석을 통해 고객의 최근 구매일(Recency), 구매 빈도(Frequency), 구매 금액(Monetary)를 기준으로 고객들을 분류하고, K-Means 군집화 알고리즘을 적용하여 3개의 주요 클러스터를 식별하는 작업을 진행했다.

   Cluster 0 (VIP 고객): 높은 빈도와 금액으로 구매를 자주 하는 고객 그룹. 이들에게는 로열티 프로그램과 특별한 혜택을 제공하는 전략이 적합하다.
 
   Cluster 1 (이탈 가능 고객): 구매 빈도와 금액이 낮고, 구매한 지 오래된 고객 그룹으로, 재구매 유도 캠페인을 통해 이탈을 방지하는 것이 중요하다.
 
   Cluster 2 (잠재 고객): 중간 정도의 구매 빈도와 금액을 가진 고객 그룹으로, 적극적인 마케팅 전략을 통해 VIP 고객으로 전환할 수 있는 잠재력이 있다.
 
  다음과 같은 결과를 도출해 내었다. 뿐만 아니라 도출해낸 결과를 토대로 각 고객 그룹에 어떤 마케팅 전략을 적용해야 할지에 대해서도 분석했다. 

* 6.2 한계점 및 어려웠던 부분

 이번 프로젝트에서 진행한 고객 세그먼테이션은 다양한 고객 그룹을 식별하고 그에 맞는 맞춤형 마케팅 전략을 수립하는 데 중요한 도구이지만, 몇 가지 한계점도 존재한다. 이 한계점을 잘 이해하고 대응 방안을 마련하는 것이 중요하다고 생각한다. 

  * 6.2-1 세그먼트 간 경계의 모호성

  세그먼트 간 경계가 명확하지 않는다는 것에 한계를 가지고 있다.  UCI Online Retail Data Set을 군집화하기 전에 진행했던 전처리 과정에서 데이터를 왜곡시킬 수 있는 부분은 삭제를 했지만 데이터 내에 숨어 있는 다양한 변수들이 분석 결과에 영향을 미칠 수 있다. 현재 데이터는 간단한 데이터로 간단한 데이터 처리로 어느정도 분류를 진행할 수 있었다. 하지만 군집화가 명확하게 이루어지 않은 점에서 한계점을 가지고 있다.

 * 6.2-2 데이터 수집의 어려움

  UCI Online Retail Data Set은 학습용 데이터로서는 훌륭한 데이터이다. 하지만 고객에 대해 분석할 수 있는 정보가 부족했다. 해당 데이터는 고객의 구매 기록에 집중되어 있다. 그래서 고객의 심리적이나 행동적 특성, 또는 지리적 정보와 같은 중요한 마케팅 분석 요소가 부족했다. 결론적으로 RFM 분석을 진행했다. 물론 RFM 분석역시 훌륭한 분석 방법이지만 여러 분석 방법을 적용해보지 못 했다는 것에 아쉬움을 가지고 있다.


 * 6.2-3 어려웠던 부분

이번 프로젝트를 진행하면서 몇 가지 어려운 점을 경험했다. 그러나 이러한 어려움은 결국 프로젝트의 깊이를 더하고, 새로운 관점을 배우는 기회가 되었다.

  첫 번째 어려웠던 점은 기존UCI Online Retail Data Set 분석 사례와 다르게 분석하려는 시도였다. 해당 데이터셋은 다양한 분석 사례가 존재하지만, 기존 방법론을 그대로 따르기보다는 새로운 방식으로 접근해 보고자 했다. 특히 특정 데이터를 삭제하거나 정제할 때, 단순히 관습적으로 처리하기보다는 명확한 근거와 논리적 이유에 기반해 삭제 여부를 결정했다. 이 과정에서 기존 분석 사례에 의존하는 데에 한계가 있었고, 모든 전처리 단계에서 근거를 확보해야 했기 때문에 시간이 더 많이 소요되었다. 이러한 접근 방식은 어려움을 동반했지만, 결국 데이터 분석의 본질에 대해 더 깊이 고민할 수 있게 했다. 데이터를 다룰 때는 단순히 모델의 성능을 높이는 것뿐만 아니라, 각 데이터 포인트가 분석 과정에 어떻게 영향을 미치는지에 대해 명확히 이해하는 것이 중요함을 깨달았다. 특히, 군집화를 위한 전처리 단계에서부터 데이터의 의미와 분석 근거에 대해 더 많은 공부를 하게 되었으며, 이번 프로젝트가 단순한 분석 작업 이상의 학습 과정이 되었다.

  본 프로젝트를 진행하면서 군집화와 고객 세그먼테이션이 단순한 도구가 아니라, 실제 비즈니스 의사결정에 어떻게 활용될 수 있는지를 깊이 고민하게 만들었으며, 특히 책이나 강의에서 배운 내용을 넘어 현실적인 문제 해결에 중점을 두는 분석을 진행하게 된 계기였다.

  두 번째 어려움은 분석 결과를 다른 사람에게 명확히 전달하는 것이었다. 프로젝트를 진행하면서 항상 염두에 두었던 것은 분석의 목적과 결과를 타인에게 설명하는 관점이었다. 고객 세그먼테이션을 왜 진행해야 하는지, 군집화 분석이 어떠한 의의를 가지는지, 그리고 그 결과에 따라 어떤 마케팅 전략을 제안할 수 있는지를 명확히 설명할 수 있어야 했다. 이 목표를 달성하기 위해 여러 논문과 마케팅 제안서 등을 참고하며 논리적 구조를 어떻게 잡아야 할지 고민했다. 프로젝트에서 단순한 기술적 분석을 넘어서 결과를 어떻게 전달할지, 가독성을 높이는 방법은 무엇인지에 대해 많은 고민을 했다. 아직 논리의 구성과 가독성 측면에서 부족함이 있음을 느끼고 있지만, 이번 프로젝트를 통해 데이터 분석 결과를 의사결정에 연결짓는 것의 중요성을 깨닫게 되었다.

 이러한 고민은 단순히 분석 기술을 습득하는 것 이상의 의미를 가진다고 생각한다. 데이터를 통해 실제 비즈니스 문제를 해결하려면 결과 자체만으로는 충분하지 않으며, 이를 어떻게 효과적으로 전달하고 설득할 것인가가 중요하다고 본다. 앞으로도 이러한 부분을 꾸준히 보완하고 발전시켜 나가야 한다고 생각한다.















